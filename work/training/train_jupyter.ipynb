{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (2.1.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.11/site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch) (3.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch) (2023.9.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.11/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.11/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.11/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.11/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /opt/conda/lib/python3.11/site-packages (from torch) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /opt/conda/lib/python3.11/site-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.101)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: peft in /opt/conda/lib/python3.11/site-packages (0.7.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from peft) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from peft) (23.2)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from peft) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.11/site-packages (from peft) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.11/site-packages (from peft) (2.1.2)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.11/site-packages (from peft) (4.36.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from peft) (4.66.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.11/site-packages (from peft) (0.26.1)\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.11/site-packages (from peft) (0.4.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.11/site-packages (from peft) (0.20.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.17.0->peft) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.17.0->peft) (2023.9.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.17.0->peft) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.17.0->peft) (4.8.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->peft) (12.3.101)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers->peft) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.11/site-packages (from transformers->peft) (0.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.11/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: trl in /opt/conda/lib/python3.11/site-packages (0.7.9)\n",
      "Requirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.11/site-packages (from trl) (2.1.2)\n",
      "Requirement already satisfied: transformers>=4.31.0 in /opt/conda/lib/python3.11/site-packages (from trl) (4.36.2)\n",
      "Requirement already satisfied: numpy>=1.18.2 in /opt/conda/lib/python3.11/site-packages (from trl) (1.24.4)\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.11/site-packages (from trl) (0.26.1)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.11/site-packages (from trl) (2.16.1)\n",
      "Requirement already satisfied: tyro>=0.5.11 in /opt/conda/lib/python3.11/site-packages (from trl) (0.6.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch>=1.4.0->trl) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.11/site-packages (from torch>=1.4.0->trl) (4.8.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=1.4.0->trl) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.4.0->trl) (3.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.4.0->trl) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch>=1.4.0->trl) (2023.9.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.4.0->trl) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.4.0->trl) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.4.0->trl) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.11/site-packages (from torch>=1.4.0->trl) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.11/site-packages (from torch>=1.4.0->trl) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.11/site-packages (from torch>=1.4.0->trl) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.11/site-packages (from torch>=1.4.0->trl) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.11/site-packages (from torch>=1.4.0->trl) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.11/site-packages (from torch>=1.4.0->trl) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /opt/conda/lib/python3.11/site-packages (from torch>=1.4.0->trl) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.4.0->trl) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /opt/conda/lib/python3.11/site-packages (from torch>=1.4.0->trl) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.4.0->trl) (12.3.101)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.11/site-packages (from transformers>=4.31.0->trl) (0.20.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers>=4.31.0->trl) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers>=4.31.0->trl) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers>=4.31.0->trl) (2023.12.25)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers>=4.31.0->trl) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.11/site-packages (from transformers>=4.31.0->trl) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.11/site-packages (from transformers>=4.31.0->trl) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers>=4.31.0->trl) (4.66.1)\n",
      "Requirement already satisfied: docstring-parser>=0.14.1 in /opt/conda/lib/python3.11/site-packages (from tyro>=0.5.11->trl) (0.15)\n",
      "Requirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.11/site-packages (from tyro>=0.5.11->trl) (13.7.0)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /opt/conda/lib/python3.11/site-packages (from tyro>=0.5.11->trl) (1.6.5)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from accelerate->trl) (5.9.5)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.11/site-packages (from datasets->trl) (13.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.11/site-packages (from datasets->trl) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from datasets->trl) (0.3.7)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from datasets->trl) (2.1.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.11/site-packages (from datasets->trl) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.11/site-packages (from datasets->trl) (0.70.15)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.11/site-packages (from datasets->trl) (3.8.6)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets->trl) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets->trl) (3.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets->trl) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets->trl) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets->trl) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets->trl) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets->trl) (1.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers>=4.31.0->trl) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers>=4.31.0->trl) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers>=4.31.0->trl) (2023.7.22)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.16.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.4.0->trl) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets->trl) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets->trl) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets->trl) (2023.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.11/site-packages (from sympy->torch>=1.4.0->trl) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets->trl) (1.16.0)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.11/site-packages (4.36.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.20.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.9.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: bitsandbytes in /opt/conda/lib/python3.11/site-packages (0.42.0)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.11/site-packages (from bitsandbytes) (1.11.3)\n",
      "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /opt/conda/lib/python3.11/site-packages (from scipy->bitsandbytes) (1.24.4)\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.11/site-packages (0.26.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from accelerate) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.11/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.11/site-packages (from accelerate) (2.1.2)\n",
      "Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.11/site-packages (from accelerate) (0.20.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.11/site-packages (from accelerate) (0.4.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (4.8.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (2023.9.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.3.101)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub->accelerate) (4.66.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.11/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# pip install\n",
    "!pip install torch\n",
    "!pip install peft\n",
    "!pip install trl\n",
    "!pip install transformers\n",
    "!pip install bitsandbytes\n",
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoTokenizer,\n",
    "    TextStreamer\n",
    ")\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "from trl import SFTTrainer\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mistralai/Mixtral-8x7B-Instruct-v0.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularData:\n",
    "\n",
    "    def __init__(self, paths: List, tokenizer, max_tokens, test_size = 0.05, validation_size = 0.05):\n",
    "        self.paths = paths\n",
    "        self.validation_size = validation_size\n",
    "        self.test_size = test_size\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_tokens = max_tokens\n",
    "\n",
    "    def get_current_size(self, dataset):\n",
    "        count = 0\n",
    "        with open(dataset) as f:\n",
    "            for _ in f:\n",
    "                count += 1\n",
    "        return count\n",
    "    \n",
    "    def to_jsonl(self, dataset, path):\n",
    "        with open(path, \"w\") as f:\n",
    "            for data in dataset:\n",
    "                f.write(json.dumps(data) + \"\\n\")\n",
    "\n",
    "    def find_all_entities(self):\n",
    "        all_entities_set: set = set()\n",
    "        for path in tqdm(self.paths):\n",
    "            with open(path) as f:\n",
    "                for _, data in enumerate(f):\n",
    "                    table = json.loads(data)\n",
    "                    prompt = f\"<s>[INST] {table['instruction']} {table['input']}[/INST] {table['output']}</s>\"\n",
    "                    encoded_input = self.tokenizer(prompt, add_special_tokens=False)\n",
    "                    if len(encoded_input[\"input_ids\"]) < self.max_tokens:\n",
    "                        for output_table in table[\"output\"].split(\";\"):\n",
    "                            try:\n",
    "                                all_entities_set.add(output_table.split(\"=\")[1].split(\" \")[0])\n",
    "                            except:\n",
    "                                print(output_table)\n",
    "        return all_entities_set\n",
    "    \n",
    "    def load_wikidata_training(self, path):\n",
    "        training_data = []\n",
    "        with open(path) as f:\n",
    "            for data in f:\n",
    "                prompt_wikidata = json.loads(data)\n",
    "                training_data.append(prompt_wikidata[\"prompt\"])\n",
    "        \n",
    "        return Dataset.from_dict({\"prompt\": training_data})\n",
    "\n",
    "    def load_tables(self):\n",
    "        training_data = []\n",
    "        validation_data = []\n",
    "        test_data = []\n",
    "        test_results = []\n",
    "        train_tables = []; val_tables = []; test_tables = []\n",
    "        train_dataset = []; val_dataset = []; test_dataset = []\n",
    "        for path in tqdm(self.paths):\n",
    "            current_size = self.get_current_size(path)\n",
    "            validation_size = int(current_size * self.validation_size)\n",
    "            test_size = int(current_size * self.test_size)\n",
    "            print(f\"Validation size: {validation_size}\")\n",
    "            print(f\"Test size: {test_size}\")\n",
    "            with open(path) as f:\n",
    "                for _, data in enumerate(f):\n",
    "                    table = json.loads(data)\n",
    "                    prompt = f\"<s>[INST] {table['instruction']} {table['input']}. {table['pool_instruction']} {table['pool']}[/INST] {table['output']}</s>\"\n",
    "                    encoded_input = self.tokenizer(prompt, add_special_tokens=False)\n",
    "                    if len(encoded_input[\"input_ids\"]) < self.max_tokens:\n",
    "                        if len(validation_data) < validation_size:\n",
    "                            validation_data.append(f\"<s>[INST] {table['instruction']} {table['input']} [/INST] {table['output']}</s>\")\n",
    "                            val_tables.append(table[\"table\"])\n",
    "                            val_dataset.append(table[\"dataset\"])\n",
    "                        elif len(test_data) < test_size:\n",
    "                            prompt = f\"<s>[INST] {table['instruction']} {table['input']} [/INST]\"\n",
    "                            test_data.append(prompt)\n",
    "                            test_results.append({table['output']})\n",
    "                            test_tables.append(table[\"table\"])\n",
    "                            test_dataset.append(table[\"dataset\"])\n",
    "                        else:\n",
    "                            train_tables.append(table[\"table\"])\n",
    "                            train_dataset.append(table[\"dataset\"])\n",
    "                            training_data.append(f\"<s>[INST] {table['instruction']} {table['input']} [/INST] {table['output']}</s>\")\n",
    "\n",
    "        return (\n",
    "            Dataset.from_dict({\"table\": train_tables, \"dataset\": train_dataset, \"prompt\": training_data}),\n",
    "            Dataset.from_dict({\"table\": val_tables, \"dataset\": val_dataset, \"prompt\": validation_data}),\n",
    "            Dataset.from_dict({\"table\": test_tables, \"dataset\": test_dataset, \"prompt\": test_data}),\n",
    "            Dataset.from_dict({\"table\": test_tables, \"dataset\": test_dataset, \"prompt\": test_results}),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "\n",
    "    def __init__(self, model_name):\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, add_eos_token=True, use_fast=True)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.tokenizer.pad_token_id =  self.tokenizer.eos_token_id\n",
    "        self.tokenizer.padding_side = 'right'\n",
    "\n",
    "    @property\n",
    "    def get_tokenizer(self):\n",
    "        return self.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# General parameters\n",
    "################################################################################\n",
    "\n",
    "general_config = {\n",
    "    \"new_model\": \"mistralai-sti-instruct\",\n",
    "}\n",
    "\n",
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "\n",
    "lora_config = {\n",
    "    \"lora_r\": 64,\n",
    "    \"lora_alpha\": 16,\n",
    "    \"lora_dropout\": 0.1,\n",
    "}\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "bitsandbytes = {\n",
    "    \"use_4bit\": True,\n",
    "    \"bnb_4bit_compute_dtype\": \"float16\",\n",
    "    \"bnb_4bit_quant_type\": \"nf4\",\n",
    "    \"use_nested_quant\": True,\n",
    "}\n",
    "\n",
    "################################################################################\n",
    "# TrainingArguments parameters\n",
    "################################################################################\n",
    "\n",
    "training_args = {\n",
    "    \"output_dir\": \"./results_mixtral_sft\",\n",
    "    \"evaluation_strategy\": \"epoch\",\n",
    "    \"num_train_epochs\": 1,\n",
    "    \"do_eval\": True,\n",
    "    \"fp16\": True,\n",
    "    \"bf16\": False,\n",
    "    \"per_device_train_batch_size\": 32,\n",
    "    \"per_device_eval_batch_size\": 32,\n",
    "    \"gradient_checkpointing\": False,\n",
    "    \"save_strategy\": \"epoch\",\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"optim\": \"paged_adamw_8bit\",\n",
    "    \"lr_scheduler_type\": \"linear\",\n",
    "    \"save_steps\": 25,\n",
    "    \"log_level\": \"debug\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_paths = [\n",
    "    # CEA\n",
    "    \"cea/cea_hardtables_2022_r1.jsonl\",\n",
    "    \"cea/cea_hardtables_2022_r2.jsonl\",\n",
    "    \"cea/cea_semTab_2020_r1.jsonl\",\n",
    "    \"cea/cea_semTab_2020_r2.jsonl\",\n",
    "    \"cea/cea_semTab_2020_r3.jsonl\",\n",
    "    \"cea/cea_semTab_2020_r4.jsonl\",\n",
    "    \"cea/cea_wikidata_tables_2023.jsonl\",\n",
    "    # CTA\n",
    "    \"cta/cta_hardtables_2022_r1.jsonl\",\n",
    "    \"cta/cta_hardtables_2022_r2.jsonl\",\n",
    "    \"cta/cta_semTab_2020_r1.jsonl\",\n",
    "    \"cta/cta_semTab_2020_r2.jsonl\",\n",
    "    \"cta/cta_semTab_2020_r3.jsonl\",\n",
    "    \"cta/cta_semTab_2020_r4.jsonl\",\n",
    "    \"cta/cta_wikidata_tables_2023.jsonl\",\n",
    "    #CPA\n",
    "    \"cpa/cpa_hardtables_2022_r1.jsonl\",\n",
    "    \"cpa/cpa_hardtables_2022_r2.jsonl\",\n",
    "    \"cpa/cpa_semTab_2020_r1.jsonl\",\n",
    "    \"cpa/cpa_semTab_2020_r2.jsonl\",\n",
    "    \"cpa/cpa_semTab_2020_r3.jsonl\",\n",
    "    \"cpa/cpa_semTab_2020_r4.jsonl\",\n",
    "    \"cpa/cpa_wikidata_tables_2023.jsonl\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"../datasets/output/\"\n",
    "all_paths = [base_path + path for path in relative_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_paths' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/jovyan/work/training/train_jupyter.ipynb Cella 10\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7374695f6c6c6d5f6a757079746572227d@ssh-remote%2B74.234.142.60/home/jovyan/work/training/train_jupyter.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m Tokenizer(model_name)\u001b[39m.\u001b[39mget_tokenizer\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7374695f6c6c6d5f6a757079746572227d@ssh-remote%2B74.234.142.60/home/jovyan/work/training/train_jupyter.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m tabular_data \u001b[39m=\u001b[39m TabularData(all_paths, tokenizer\u001b[39m=\u001b[39mtokenizer, max_tokens\u001b[39m=\u001b[39m\u001b[39m256\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7374695f6c6c6d5f6a757079746572227d@ssh-remote%2B74.234.142.60/home/jovyan/work/training/train_jupyter.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m training_entity \u001b[39m=\u001b[39m tabular_data\u001b[39m.\u001b[39mload_wikidata_training(\u001b[39m\"\u001b[39m\u001b[39m./pretraining_llm_wikidata.jsonl\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_paths' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(model_name).get_tokenizer\n",
    "tabular_data = TabularData(all_paths, tokenizer=tokenizer, max_tokens=256)\n",
    "training_entity = tabular_data.load_wikidata_training(\"./pretraining_llm_wikidata.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "with open(\"./entities.txt\", 'w') as fp:\n",
    "    for item in entity_set:\n",
    "        fp.write(\"%s\\n\" % item)\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation size: 5489\n",
      "Test size: 5489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [02:19<00:00, 139.26s/it]\n"
     ]
    }
   ],
   "source": [
    "# import dataset\n",
    "all_paths = [\"../datasets/output/pool/cea_pool_2.jsonl\"]\n",
    "tokenizer = Tokenizer(model_name).get_tokenizer\n",
    "tabular_data = TabularData(all_paths, tokenizer=tokenizer, max_tokens=550)\n",
    "training_data, validation_data, test_data, test_results = tabular_data.load_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8868, 5489, 5489)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data), len(validation_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_data.to_jsonl(training_data, \"./datasets_new/QID&Labels/training_data.jsonl\")\n",
    "tabular_data.to_jsonl(validation_data, \"./datasets_new/QID&Labels/validation_data.jsonl\")\n",
    "tabular_data.to_jsonl(test_data, \"./datasets_new/QID&Labels/test_data.jsonl\")\n",
    "tabular_data.to_jsonl(test_results, \"./datasets_new/QID&Labels/test_results.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>[INST] perform the cell entity annotation (cea) task on this table: Mountain Fire Lookout Tower;place listed on the National Register of Historic Places;Wisconsin;1254;fire lookout tower;United States of America|Mount Tremper Fire Observation Station;place listed on the National Register of Historic Places;New York;2720;fire lookout tower;United States of America|Mount Adams Fire Observation Station;place listed on the National Register of Historic Places;New York;3520;fire lookout tower;United States of America|Monjeau Lookout;place listed on the National Register of Historic Places;New Mexico;9582;fire lookout tower;United States of America|Loon Lake Mountain Fire Observation Station;place listed on the National Register of Historic Places;New York;3332;fire lookout tower;United States of America. use the following pool of entities to annotate the table: Q49046807 [loon lake mountain];Q16979539 [fifield fire lookout tower];Q748998 [fire lookout tower];Q19558910 [national register of historic places listed place];Q49433534 [monjeau number 1 claim];Q6924190 [mount tremper];Q2039364 [new york new york];Q49050982 [monjeau peak];Q4852537 [balsam lake mountain fire observation station];Q30 [united states of america];Q1522 [new mexico];Q103971355 [united states of america];Q6675645 [loon lake mountain fire observation station];Q1329425 [fire lookout tower (szécsény)];Q86925961 [english mountain fire lookout tower];Q6924192 [mount tremper fire observation station];Q5365167 [united states of america];Q86928324 [horn mountain fire lookout tower];Q6924189 [mount tremper];Q2629763 [new mexico];Q3339035 [new york new york];Q527916 [new york new york];Q6123624 [jails and prisons listed on the national register of historic places];Q6900549 [monjeau lookout];Q1384 [new york];Q60748655 [mount gambier fire station];Q8665650 [kategorie:vorlage:navigationsleiste national register of historic places];Q597711 [new mexico];Q1537 [wisconsin];Q105870683 [united states of america];Q6919254 [mount adams fire observation station];Q913413 [wisconsin dells, wisconsin];Q10862958 [sjabloon:infobox national register of historic places];Q1003702 [wisconsin rapids, wisconsin];Q6924944 [mountain fire lookout tower];[/INST] (1,0)=Q6924944 [mountain fire lookout tower]|(1,1)=Q19558910 [national register of historic places listed place]|(1,2)=Q1537 [wisconsin]|(1,4)=Q748998 [fire lookout tower]|(1,5)=Q30 [united states of america]|(2,0)=Q6924192 [mount tremper fire observation station]|(2,1)=Q19558910 [national register of historic places listed place]|(2,2)=Q1384 [new york]|(2,4)=Q748998 [fire lookout tower]|(2,5)=Q30 [united states of america]|(3,0)=Q6919254 [mount adams fire observation station]|(3,1)=Q19558910 [national register of historic places listed place]|(3,2)=Q1384 [new york]|(3,4)=Q748998 [fire lookout tower]|(3,5)=Q30 [united states of america]|(4,0)=Q6900549 [monjeau lookout]|(4,1)=Q19558910 [national register of historic places listed place]|(4,2)=Q1522 [new mexico]|(4,4)=Q748998 [fire lookout tower]|(4,5)=Q30 [united states of america]|(5,0)=Q6675645 [loon lake mountain fire observation station]|(5,1)=Q19558910 [national register of historic places listed place]|(5,2)=Q1384 [new york]|(5,4)=Q748998 [fire lookout tower]|(5,5)=Q30 [united states of america]</s>'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 127016/127016 [00:31<00:00, 4090.05it/s]\n",
      "100%|██████████| 15818/15818 [00:03<00:00, 4047.89it/s]\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "max_enc_input = 0\n",
    "max_character_length = 0\n",
    "for data in tqdm(training_data):\n",
    "    encoded_input = tokenizer(data[\"prompt\"], add_special_tokens=False)\n",
    "    max_enc_input = max(max_enc_input, len(encoded_input[\"input_ids\"]))\n",
    "    max_character_length = max(max_character_length, len(data[\"prompt\"]))\n",
    "    count += 1\n",
    "\n",
    "for data in tqdm(validation_data):\n",
    "    encoded_input = tokenizer(data[\"prompt\"], add_special_tokens=False)\n",
    "    max_enc_input = max(max_enc_input, len(encoded_input[\"input_ids\"]))\n",
    "    max_character_length = max(max_character_length, len(data[\"prompt\"]))\n",
    "    count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142834 1139 255\n"
     ]
    }
   ],
   "source": [
    "print(count, max_character_length, max_enc_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BitsAndBytesConfig\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "   bnb_4bit_compute_dtype=\"float16\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/jovyan/work/training/train_jupyter.ipynb Cella 18\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7374695f6c6c6d5f6a757079746572227d@ssh-remote%2B74.234.142.60/home/jovyan/work/training/train_jupyter.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# load model\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7374695f6c6c6d5f6a757079746572227d@ssh-remote%2B74.234.142.60/home/jovyan/work/training/train_jupyter.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(model_name, quantization_config\u001b[39m=\u001b[39;49mbnb_config, device_map\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mauto\u001b[39;49m\u001b[39m'\u001b[39;49m, use_cache\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7374695f6c6c6d5f6a757079746572227d@ssh-remote%2B74.234.142.60/home/jovyan/work/training/train_jupyter.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m model \u001b[39m=\u001b[39m prepare_model_for_kbit_training(model)\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7374695f6c6c6d5f6a757079746572227d@ssh-remote%2B74.234.142.60/home/jovyan/work/training/train_jupyter.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mpad_token_id \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mpad_token_id\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:566\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    565\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 566\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    567\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    568\u001b[0m     )\n\u001b[1;32m    569\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    570\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    571\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    572\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/modeling_utils.py:3476\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3473\u001b[0m     keep_in_fp32_modules \u001b[39m=\u001b[39m []\n\u001b[1;32m   3475\u001b[0m \u001b[39mif\u001b[39;00m load_in_8bit \u001b[39mor\u001b[39;00m load_in_4bit:\n\u001b[0;32m-> 3476\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mintegrations\u001b[39;00m \u001b[39mimport\u001b[39;00m get_keys_to_not_convert, replace_with_bnb_linear\n\u001b[1;32m   3478\u001b[0m     llm_int8_skip_modules \u001b[39m=\u001b[39m quantization_config\u001b[39m.\u001b[39mllm_int8_skip_modules\n\u001b[1;32m   3479\u001b[0m     load_in_8bit_fp32_cpu_offload \u001b[39m=\u001b[39m quantization_config\u001b[39m.\u001b[39mllm_int8_enable_fp32_cpu_offload\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1229\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/utils/import_utils.py:1372\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1370\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_module(name)\n\u001b[1;32m   1371\u001b[0m \u001b[39melif\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_class_to_module\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m-> 1372\u001b[0m     module \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_module(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_class_to_module[name])\n\u001b[1;32m   1373\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(module, name)\n\u001b[1;32m   1374\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/utils/import_utils.py:1382\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1380\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_module\u001b[39m(\u001b[39mself\u001b[39m, module_name: \u001b[39mstr\u001b[39m):\n\u001b[1;32m   1381\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1382\u001b[0m         \u001b[39mreturn\u001b[39;00m importlib\u001b[39m.\u001b[39;49mimport_module(\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m module_name, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m)\n\u001b[1;32m   1383\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1384\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1385\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to import \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mmodule_name\u001b[39m}\u001b[39;00m\u001b[39m because of the following error (look up to see its\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1386\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m traceback):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1387\u001b[0m         ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/integrations/bitsandbytes.py:11\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m is_accelerate_available, is_bitsandbytes_available, logging\n\u001b[1;32m     10\u001b[0m \u001b[39mif\u001b[39;00m is_bitsandbytes_available():\n\u001b[0;32m---> 11\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mbitsandbytes\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mbnb\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnn\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/bitsandbytes/__init__.py:16\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mautograd\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_functions\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      8\u001b[0m     MatmulLtState,\n\u001b[1;32m      9\u001b[0m     bmm_cublas,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     matmul_4bit\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcextension\u001b[39;00m \u001b[39mimport\u001b[39;00m COMPILED_WITH_CUDA\n\u001b[0;32m---> 16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mimport\u001b[39;00m modules\n\u001b[1;32m     18\u001b[0m \u001b[39mif\u001b[39;00m COMPILED_WITH_CUDA:\n\u001b[1;32m     19\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39moptim\u001b[39;00m \u001b[39mimport\u001b[39;00m adam\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/bitsandbytes/nn/__init__.py:6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Copyright (c) Facebook, Inc. and its affiliates.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# This source code is licensed under the MIT license found in the\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# LICENSE file in the root directory of this source tree.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmodules\u001b[39;00m \u001b[39mimport\u001b[39;00m Int8Params, Linear8bitLt, StableEmbedding, Linear4bit, LinearNF4, LinearFP4, Params4bit, OutlierAwareLinear, SwitchBackLinearBnb, Embedding\n\u001b[0;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mtriton_based_modules\u001b[39;00m \u001b[39mimport\u001b[39;00m SwitchBackLinear, SwitchBackLinearGlobal, SwitchBackLinearVectorwise, StandardLinear\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/bitsandbytes/nn/triton_based_modules.py:8\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfunctools\u001b[39;00m \u001b[39mimport\u001b[39;00m partial\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbitsandbytes\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtriton\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtriton_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m is_triton_available\n\u001b[0;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbitsandbytes\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtriton\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdequantize_rowwise\u001b[39;00m \u001b[39mimport\u001b[39;00m dequantize_rowwise\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbitsandbytes\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtriton\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mquantize_rowwise\u001b[39;00m \u001b[39mimport\u001b[39;00m quantize_rowwise\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbitsandbytes\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtriton\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mquantize_columnwise_and_transpose\u001b[39;00m \u001b[39mimport\u001b[39;00m quantize_columnwise_and_transpose\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/bitsandbytes/triton/dequantize_rowwise.py:10\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mdequantize_rowwise\u001b[39m(x: torch\u001b[39m.\u001b[39mTensor, state_x: torch\u001b[39m.\u001b[39mTensor): \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 10\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mtriton\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mtriton\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlanguage\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtl\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mtriton\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmatmul_perf_model\u001b[39;00m \u001b[39mimport\u001b[39;00m early_config_prune, estimate_matmul_time\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/triton/__init__.py:20\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mruntime\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      9\u001b[0m     autotune,\n\u001b[1;32m     10\u001b[0m     Config,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m     MockTensor,\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mruntime\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mjit\u001b[39;00m \u001b[39mimport\u001b[39;00m jit\n\u001b[0;32m---> 20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcompiler\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39mcompile\u001b[39m, CompilationError\n\u001b[1;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m language\n\u001b[1;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m testing\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/triton/compiler/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcompiler\u001b[39;00m \u001b[39mimport\u001b[39;00m CompiledKernel, \u001b[39mcompile\u001b[39m, instance_descriptor\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39merrors\u001b[39;00m \u001b[39mimport\u001b[39;00m CompilationError\n\u001b[1;32m      4\u001b[0m __all__ \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mcompile\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39minstance_descriptor\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mCompiledKernel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mCompilationError\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/triton/compiler/compiler.py:27\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mruntime\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mjit\u001b[39;00m \u001b[39mimport\u001b[39;00m (JITFunction, get_cuda_stream, get_current_device,\n\u001b[1;32m     25\u001b[0m                            get_device_capability, version_key)\n\u001b[1;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtools\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdisasm\u001b[39;00m \u001b[39mimport\u001b[39;00m extract\n\u001b[0;32m---> 27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcode_generator\u001b[39;00m \u001b[39mimport\u001b[39;00m ast_to_ttir\n\u001b[1;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmake_launcher\u001b[39;00m \u001b[39mimport\u001b[39;00m make_stub\n\u001b[1;32m     31\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minline_triton_ir\u001b[39m(mod):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/triton/compiler/code_generator.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwarnings\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Any, Callable, Dict, Optional, Tuple, Type, Union\n\u001b[0;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m language\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_C\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlibtriton\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtriton\u001b[39;00m \u001b[39mimport\u001b[39;00m ir\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlanguage\u001b[39;00m \u001b[39mimport\u001b[39;00m constexpr, tensor\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/triton/language/__init__.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m\"\"\"isort:skip_file\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# Import order is significant here.\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m math\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m extra\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mstandard\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      7\u001b[0m     cdiv,\n\u001b[1;32m      8\u001b[0m     sigmoid,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     zeros_like,\n\u001b[1;32m     14\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/triton/language/math.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mfunctools\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m core\n\u001b[1;32m      7\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mlru_cache()\n\u001b[1;32m      8\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlibdevice_path\u001b[39m():\n\u001b[1;32m      9\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/triton/language/core.py:1375\u001b[0m\n\u001b[1;32m   1370\u001b[0m     rvalue, rindices \u001b[39m=\u001b[39m reduce((\u001b[39minput\u001b[39m, index), axis, combine_fn,\n\u001b[1;32m   1371\u001b[0m                               _builder\u001b[39m=\u001b[39m_builder, _generator\u001b[39m=\u001b[39m_generator)\n\u001b[1;32m   1372\u001b[0m     \u001b[39mreturn\u001b[39;00m rvalue, rindices\n\u001b[0;32m-> 1375\u001b[0m \u001b[39m@jit\u001b[39;49m\n\u001b[1;32m   1376\u001b[0m \u001b[39mdef\u001b[39;49;00m \u001b[39mminimum\u001b[39;49m(x, y):\n\u001b[1;32m   1377\u001b[0m \u001b[39m    \u001b[39;49m\u001b[39m\"\"\"\u001b[39;49;00m\n\u001b[1;32m   1378\u001b[0m \u001b[39m    Computes the element-wise minimum of :code:`x` and :code:`y`.\u001b[39;49;00m\n\u001b[1;32m   1379\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1383\u001b[0m \u001b[39m    :type other: Block\u001b[39;49;00m\n\u001b[1;32m   1384\u001b[0m \u001b[39m    \"\"\"\u001b[39;49;00m\n\u001b[1;32m   1385\u001b[0m     \u001b[39mreturn\u001b[39;49;00m where(x \u001b[39m<\u001b[39;49m y, x, y)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/triton/runtime/jit.py:542\u001b[0m, in \u001b[0;36mjit\u001b[0;34m(fn, version, do_not_specialize, debug, noinline, interpret)\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[39mreturn\u001b[39;00m JITFunction(\n\u001b[1;32m    535\u001b[0m             fn,\n\u001b[1;32m    536\u001b[0m             version\u001b[39m=\u001b[39mversion,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    539\u001b[0m             noinline\u001b[39m=\u001b[39mnoinline,\n\u001b[1;32m    540\u001b[0m         )\n\u001b[1;32m    541\u001b[0m \u001b[39mif\u001b[39;00m fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 542\u001b[0m     \u001b[39mreturn\u001b[39;00m decorator(fn)\n\u001b[1;32m    544\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    545\u001b[0m     \u001b[39mreturn\u001b[39;00m decorator\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/triton/runtime/jit.py:534\u001b[0m, in \u001b[0;36mjit.<locals>.decorator\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m    532\u001b[0m     \u001b[39mreturn\u001b[39;00m GridSelector(fn)\n\u001b[1;32m    533\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 534\u001b[0m     \u001b[39mreturn\u001b[39;00m JITFunction(\n\u001b[1;32m    535\u001b[0m         fn,\n\u001b[1;32m    536\u001b[0m         version\u001b[39m=\u001b[39;49mversion,\n\u001b[1;32m    537\u001b[0m         do_not_specialize\u001b[39m=\u001b[39;49mdo_not_specialize,\n\u001b[1;32m    538\u001b[0m         debug\u001b[39m=\u001b[39;49mdebug,\n\u001b[1;32m    539\u001b[0m         noinline\u001b[39m=\u001b[39;49mnoinline,\n\u001b[1;32m    540\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/triton/runtime/jit.py:433\u001b[0m, in \u001b[0;36mJITFunction.__init__\u001b[0;34m(self, fn, version, do_not_specialize, debug, noinline)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconstexprs \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39marg_names\u001b[39m.\u001b[39mindex(name) \u001b[39mfor\u001b[39;00m name, ty \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__annotations__\u001b[39m\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mconstexpr\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m ty]\n\u001b[1;32m    432\u001b[0m \u001b[39m# launcher\u001b[39;00m\n\u001b[0;32m--> 433\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_launcher()\n\u001b[1;32m    434\u001b[0m \u001b[39m# re-use docs of wrapped function\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__doc__\u001b[39m \u001b[39m=\u001b[39m fn\u001b[39m.\u001b[39m\u001b[39m__doc__\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/triton/runtime/jit.py:388\u001b[0m, in \u001b[0;36mJITFunction._make_launcher\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    317\u001b[0m         args_signature \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(name \u001b[39mif\u001b[39;00m dflt \u001b[39m==\u001b[39m inspect\u001b[39m.\u001b[39m_empty \u001b[39melse\u001b[39;00m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m = \u001b[39m\u001b[39m{\u001b[39;00mdflt\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39mfor\u001b[39;00m name, dflt \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39marg_names, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39marg_defaults))\n\u001b[1;32m    319\u001b[0m         src \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m    320\u001b[0m \u001b[39mdef \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00margs_signature\u001b[39m}\u001b[39;00m\u001b[39m, grid=None, num_warps=4, num_stages=3, extern_libs=None, stream=None, warmup=False, device=None, device_type=None):\u001b[39m\n\u001b[1;32m    321\u001b[0m \u001b[39m    from ..compiler import compile, CompiledKernel\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[39m      return None\u001b[39m\n\u001b[1;32m    387\u001b[0m \u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m--> 388\u001b[0m         scope \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mversion_key\u001b[39m\u001b[39m\"\u001b[39m: version_key(),\n\u001b[1;32m    389\u001b[0m                  \u001b[39m\"\u001b[39m\u001b[39mget_cuda_stream\u001b[39m\u001b[39m\"\u001b[39m: get_cuda_stream,\n\u001b[1;32m    390\u001b[0m                  \u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m,\n\u001b[1;32m    391\u001b[0m                  \u001b[39m\"\u001b[39m\u001b[39m_spec_of\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_spec_of,\n\u001b[1;32m    392\u001b[0m                  \u001b[39m\"\u001b[39m\u001b[39m_key_of\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_key_of,\n\u001b[1;32m    393\u001b[0m                  \u001b[39m\"\u001b[39m\u001b[39m_device_of\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_device_of,\n\u001b[1;32m    394\u001b[0m                  \u001b[39m\"\u001b[39m\u001b[39m_pinned_memory_of\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pinned_memory_of,\n\u001b[1;32m    395\u001b[0m                  \u001b[39m\"\u001b[39m\u001b[39mcache\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache,\n\u001b[1;32m    396\u001b[0m                  \u001b[39m\"\u001b[39m\u001b[39m__spec__\u001b[39m\u001b[39m\"\u001b[39m: __spec__,\n\u001b[1;32m    397\u001b[0m                  \u001b[39m\"\u001b[39m\u001b[39mget_backend\u001b[39m\u001b[39m\"\u001b[39m: get_backend,\n\u001b[1;32m    398\u001b[0m                  \u001b[39m\"\u001b[39m\u001b[39mget_current_device\u001b[39m\u001b[39m\"\u001b[39m: get_current_device,\n\u001b[1;32m    399\u001b[0m                  \u001b[39m\"\u001b[39m\u001b[39mset_current_device\u001b[39m\u001b[39m\"\u001b[39m: set_current_device}\n\u001b[1;32m    400\u001b[0m         exec(src, scope)\n\u001b[1;32m    401\u001b[0m         \u001b[39mreturn\u001b[39;00m scope[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/triton/runtime/jit.py:113\u001b[0m, in \u001b[0;36mversion_key\u001b[0;34m()\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[39m# backend\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(TRITON_PATH, \u001b[39m\"\u001b[39m\u001b[39m_C/libtriton.so\u001b[39m\u001b[39m\"\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m--> 113\u001b[0m     contents \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [hashlib\u001b[39m.\u001b[39mmd5(f\u001b[39m.\u001b[39;49mread())\u001b[39m.\u001b[39mhexdigest()]\n\u001b[1;32m    114\u001b[0m \u001b[39m# language\u001b[39;00m\n\u001b[1;32m    115\u001b[0m language_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(TRITON_PATH, \u001b[39m'\u001b[39m\u001b[39mlanguage\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, device_map='auto', use_cache=False)\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LoRA config\n",
    "peft_config = LoraConfig(\n",
    "        lora_alpha=lora_config[\"lora_alpha\"],\n",
    "        lora_dropout=lora_config[\"lora_dropout\"],\n",
    "        r=lora_config[\"lora_r\"],\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules= [\n",
    "                \"q_proj\",\n",
    "                \"k_proj\",\n",
    "                \"v_proj\",\n",
    "                \"o_proj\",\n",
    "                \"gate_proj\",\n",
    "                \"up_proj\",\n",
    "                \"down_proj\",\n",
    "                \"lm_head\",\n",
    "        ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "198d330ac9464c4f92af505e00d601fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/127016 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62890df4ee4547feaec4f40e944a4c58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15818 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using auto half precision backend\n",
      "Currently training with a batch size of: 32\n",
      "***** Running training *****\n",
      "  Num examples = 127,016\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3,970\n",
      "  Number of trainable parameters = 56,836,096\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/opt/conda/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3970' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   3/3970 00:06 < 7:41:17, 0.14 it/s, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/jovyan/work/training/train_jupyter.ipynb Cella 18\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7374695f6c6c6d5f6a757079746572227d@ssh-remote%2B74.234.142.60/home/jovyan/work/training/train_jupyter.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m training_arguments \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7374695f6c6c6d5f6a757079746572227d@ssh-remote%2B74.234.142.60/home/jovyan/work/training/train_jupyter.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     output_dir\u001b[39m=\u001b[39mtraining_args[\u001b[39m\"\u001b[39m\u001b[39moutput_dir\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7374695f6c6c6d5f6a757079746572227d@ssh-remote%2B74.234.142.60/home/jovyan/work/training/train_jupyter.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     evaluation_strategy\u001b[39m=\u001b[39mtraining_args[\u001b[39m\"\u001b[39m\u001b[39mevaluation_strategy\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7374695f6c6c6d5f6a757079746572227d@ssh-remote%2B74.234.142.60/home/jovyan/work/training/train_jupyter.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     lr_scheduler_type\u001b[39m=\u001b[39mtraining_args[\u001b[39m\"\u001b[39m\u001b[39mlr_scheduler_type\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7374695f6c6c6d5f6a757079746572227d@ssh-remote%2B74.234.142.60/home/jovyan/work/training/train_jupyter.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7374695f6c6c6d5f6a757079746572227d@ssh-remote%2B74.234.142.60/home/jovyan/work/training/train_jupyter.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m trainer \u001b[39m=\u001b[39m SFTTrainer(\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7374695f6c6c6d5f6a757079746572227d@ssh-remote%2B74.234.142.60/home/jovyan/work/training/train_jupyter.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m         model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7374695f6c6c6d5f6a757079746572227d@ssh-remote%2B74.234.142.60/home/jovyan/work/training/train_jupyter.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m         train_dataset\u001b[39m=\u001b[39mtraining_data,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7374695f6c6c6d5f6a757079746572227d@ssh-remote%2B74.234.142.60/home/jovyan/work/training/train_jupyter.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m         args\u001b[39m=\u001b[39mtraining_arguments,\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7374695f6c6c6d5f6a757079746572227d@ssh-remote%2B74.234.142.60/home/jovyan/work/training/train_jupyter.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7374695f6c6c6d5f6a757079746572227d@ssh-remote%2B74.234.142.60/home/jovyan/work/training/train_jupyter.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7374695f6c6c6d5f6a757079746572227d@ssh-remote%2B74.234.142.60/home/jovyan/work/training/train_jupyter.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m trainer\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39msave_pretrained(general_config[\u001b[39m\"\u001b[39m\u001b[39mnew_model\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:317\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneftune_noise_alpha \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_trainer_supports_neftune:\n\u001b[1;32m    315\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_trl_activate_neftune(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel)\n\u001b[0;32m--> 317\u001b[0m output \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mtrain(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    319\u001b[0m \u001b[39m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \u001b[39m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneftune_noise_alpha \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_trainer_supports_neftune:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/trainer.py:1537\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1535\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1536\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1537\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1538\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1539\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1540\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1541\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1542\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/trainer.py:1854\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1851\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_begin(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[1;32m   1853\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1854\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1856\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1857\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1858\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1859\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1860\u001b[0m ):\n\u001b[1;32m   1861\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1862\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/trainer.py:2744\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2742\u001b[0m         scaled_loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m   2743\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2744\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maccelerator\u001b[39m.\u001b[39;49mbackward(loss)\n\u001b[1;32m   2746\u001b[0m \u001b[39mreturn\u001b[39;00m loss\u001b[39m.\u001b[39mdetach() \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/accelerate/accelerator.py:1962\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1960\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   1961\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1962\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscaler\u001b[39m.\u001b[39;49mscale(loss)\u001b[39m.\u001b[39;49mbackward(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1963\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1964\u001b[0m     loss\u001b[39m.\u001b[39mbackward(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train model\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=training_args[\"output_dir\"],\n",
    "    evaluation_strategy=training_args[\"evaluation_strategy\"],\n",
    "    do_eval=training_args[\"do_eval\"],\n",
    "    optim=training_args[\"optim\"],\n",
    "    fp16=training_args[\"fp16\"],\n",
    "    bf16=training_args[\"bf16\"],\n",
    "    per_device_train_batch_size=training_args[\"per_device_train_batch_size\"],\n",
    "    per_device_eval_batch_size=training_args[\"per_device_eval_batch_size\"],\n",
    "    log_level=training_args[\"log_level\"],\n",
    "    save_strategy=training_args[\"save_strategy\"],\n",
    "    save_steps=training_args[\"save_steps\"],\n",
    "    num_train_epochs=training_args[\"num_train_epochs\"],\n",
    "    learning_rate=training_args[\"learning_rate\"],\n",
    "    lr_scheduler_type=training_args[\"lr_scheduler_type\"],\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=training_data,\n",
    "    eval_dataset=validation_data,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"prompt\",\n",
    "    max_seq_length=256,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.model.save_pretrained(general_config[\"new_model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14d84138863046578505a13f3d9f13e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Can't find 'adapter_config.json' at 'mistralai-sti-instruct'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:286\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 286\u001b[0m     response\u001b[39m.\u001b[39;49mraise_for_status()\n\u001b[1;32m    287\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[39mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/mistralai-sti-instruct/resolve/main/adapter_config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/peft/config.py:184\u001b[0m, in \u001b[0;36mPeftConfigMixin._get_peft_type\u001b[0;34m(cls, model_id, **hf_hub_download_kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 184\u001b[0m     config_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[1;32m    185\u001b[0m         model_id,\n\u001b[1;32m    186\u001b[0m         CONFIG_NAME,\n\u001b[1;32m    187\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhf_hub_download_kwargs,\n\u001b[1;32m    188\u001b[0m     )\n\u001b[1;32m    189\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:1368\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[1;32m   1366\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(head_call_error, RepositoryNotFoundError) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(head_call_error, GatedRepoError):\n\u001b[1;32m   1367\u001b[0m     \u001b[39m# Repo not found => let's raise the actual error\u001b[39;00m\n\u001b[0;32m-> 1368\u001b[0m     \u001b[39mraise\u001b[39;00m head_call_error\n\u001b[1;32m   1369\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1370\u001b[0m     \u001b[39m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:1238\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[1;32m   1237\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1238\u001b[0m     metadata \u001b[39m=\u001b[39m get_hf_file_metadata(\n\u001b[1;32m   1239\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m   1240\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m   1241\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1242\u001b[0m         timeout\u001b[39m=\u001b[39;49metag_timeout,\n\u001b[1;32m   1243\u001b[0m         library_name\u001b[39m=\u001b[39;49mlibrary_name,\n\u001b[1;32m   1244\u001b[0m         library_version\u001b[39m=\u001b[39;49mlibrary_version,\n\u001b[1;32m   1245\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m   1246\u001b[0m     )\n\u001b[1;32m   1247\u001b[0m \u001b[39mexcept\u001b[39;00m EntryNotFoundError \u001b[39mas\u001b[39;00m http_error:\n\u001b[1;32m   1248\u001b[0m     \u001b[39m# Cache the non-existence of the file and raise\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:1631\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent)\u001b[0m\n\u001b[1;32m   1630\u001b[0m \u001b[39m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1631\u001b[0m r \u001b[39m=\u001b[39m _request_wrapper(\n\u001b[1;32m   1632\u001b[0m     method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mHEAD\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1633\u001b[0m     url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m   1634\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m   1635\u001b[0m     allow_redirects\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   1636\u001b[0m     follow_relative_redirects\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1637\u001b[0m     proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1638\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m   1639\u001b[0m )\n\u001b[1;32m   1640\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:385\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[39mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 385\u001b[0m     response \u001b[39m=\u001b[39m _request_wrapper(\n\u001b[1;32m    386\u001b[0m         method\u001b[39m=\u001b[39;49mmethod,\n\u001b[1;32m    387\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    388\u001b[0m         follow_relative_redirects\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    389\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams,\n\u001b[1;32m    390\u001b[0m     )\n\u001b[1;32m    392\u001b[0m     \u001b[39m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    393\u001b[0m     \u001b[39m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:409\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    408\u001b[0m response \u001b[39m=\u001b[39m get_session()\u001b[39m.\u001b[39mrequest(method\u001b[39m=\u001b[39mmethod, url\u001b[39m=\u001b[39murl, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams)\n\u001b[0;32m--> 409\u001b[0m hf_raise_for_status(response)\n\u001b[1;32m    410\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:323\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    315\u001b[0m     message \u001b[39m=\u001b[39m (\n\u001b[1;32m    316\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39mstatus_code\u001b[39m}\u001b[39;00m\u001b[39m Client Error.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    317\u001b[0m         \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m make sure you are authenticated.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 323\u001b[0m     \u001b[39mraise\u001b[39;00m RepositoryNotFoundError(message, response) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    325\u001b[0m \u001b[39melif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m400\u001b[39m:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-65af9a4c-6188c3f468d38fb9751db2f1;a5ebc36e-783f-44e2-8f89-48e080215bd6)\n\nRepository Not Found for url: https://huggingface.co/mistralai-sti-instruct/resolve/main/adapter_config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/jovyan/work/training/train_jupyter.ipynb Cella 21\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7374695f6c6c6d5f6a757079746572227d@ssh-remote%2B74.234.142.60/home/jovyan/work/training/train_jupyter.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Reload the base model\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7374695f6c6c6d5f6a757079746572227d@ssh-remote%2B74.234.142.60/home/jovyan/work/training/train_jupyter.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m base_model_reload \u001b[39m=\u001b[39m model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39m./mistralai-sti-instruct_Wikidata_Pretraining\u001b[39m\u001b[39m\"\u001b[39m, quantization_config\u001b[39m=\u001b[39mbnb_config, device_map\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m'\u001b[39m, use_cache\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7374695f6c6c6d5f6a757079746572227d@ssh-remote%2B74.234.142.60/home/jovyan/work/training/train_jupyter.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m model \u001b[39m=\u001b[39m PeftModel\u001b[39m.\u001b[39;49mfrom_pretrained(base_model_reload, general_config[\u001b[39m\"\u001b[39;49m\u001b[39mnew_model\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7374695f6c6c6d5f6a757079746572227d@ssh-remote%2B74.234.142.60/home/jovyan/work/training/train_jupyter.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m trained_model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mmerge_and_unload()\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7374695f6c6c6d5f6a757079746572227d@ssh-remote%2B74.234.142.60/home/jovyan/work/training/train_jupyter.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Reload tokenizer\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/peft/peft_model.py:324\u001b[0m, in \u001b[0;36mPeftModel.from_pretrained\u001b[0;34m(cls, model, model_id, adapter_name, is_trainable, config, **kwargs)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[39m# load the config\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[39mif\u001b[39;00m config \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     config \u001b[39m=\u001b[39m PEFT_TYPE_TO_CONFIG_MAPPING[\n\u001b[0;32m--> 324\u001b[0m         PeftConfig\u001b[39m.\u001b[39;49m_get_peft_type(\n\u001b[1;32m    325\u001b[0m             model_id,\n\u001b[1;32m    326\u001b[0m             subfolder\u001b[39m=\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39msubfolder\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    327\u001b[0m             revision\u001b[39m=\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mrevision\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    328\u001b[0m             cache_dir\u001b[39m=\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcache_dir\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    329\u001b[0m             use_auth_token\u001b[39m=\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39muse_auth_token\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    330\u001b[0m             token\u001b[39m=\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mtoken\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    331\u001b[0m         )\n\u001b[1;32m    332\u001b[0m     ]\u001b[39m.\u001b[39mfrom_pretrained(model_id, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    333\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(config, PeftConfig):\n\u001b[1;32m    334\u001b[0m     config\u001b[39m.\u001b[39minference_mode \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m is_trainable\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/peft/config.py:190\u001b[0m, in \u001b[0;36mPeftConfigMixin._get_peft_type\u001b[0;34m(cls, model_id, **hf_hub_download_kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m         config_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[1;32m    185\u001b[0m             model_id,\n\u001b[1;32m    186\u001b[0m             CONFIG_NAME,\n\u001b[1;32m    187\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhf_hub_download_kwargs,\n\u001b[1;32m    188\u001b[0m         )\n\u001b[1;32m    189\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mCONFIG_NAME\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m at \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mmodel_id\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    192\u001b[0m loaded_attributes \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mfrom_json_file(config_file)\n\u001b[1;32m    193\u001b[0m \u001b[39mreturn\u001b[39;00m loaded_attributes[\u001b[39m\"\u001b[39m\u001b[39mpeft_type\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: Can't find 'adapter_config.json' at 'mistralai-sti-instruct'"
     ]
    }
   ],
   "source": [
    "# Reload the base model\n",
    "base_model_reload = model = AutoModelForCausalLM.from_pretrained(\"./mistralai-sti-instruct_Wikidata_Pretraining\", quantization_config=bnb_config, device_map='auto', use_cache=False)\n",
    "model = PeftModel.from_pretrained(base_model_reload, general_config[\"new_model\"])\n",
    "trained_model = model.merge_and_unload()\n",
    "\n",
    "# Reload tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "prompt = \"<s>[INST] conduct the cell entity annotation task for wikidata on this table: milan;lombardy|rome;lazio[/INST]\"\n",
    "def stream(trained_model, prompt):\n",
    "    runtimeFlag = \"cuda:0\"\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(runtimeFlag)\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "    _ = trained_model.generate(**inputs, streamer=streamer, max_new_tokens=200)\n",
    "\n",
    "stream(trained_model, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = []\n",
    "with open(\"./datasets_new/Labels/test_data.jsonl\") as f:\n",
    "    for index, data in enumerate(f):\n",
    "        loaded_data = json.loads(data)\n",
    "        prompts.append(loaded_data)\n",
    "        if index == 2:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_input = [f\"{p['prompt']}</s>\" for p in prompts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4981f7a2f20445b4a53ea5a049528b0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/generation/utils.py:1518: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    }
   ],
   "source": [
    "trained_model = AutoModelForCausalLM.from_pretrained(\"./mistralai-sti-instruct_Labels\", quantization_config=bnb_config, device_map='auto', use_cache=False)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "encoded_input = tokenizer(prompts_input, return_tensors=\"pt\", padding=True, add_special_tokens=False)\n",
    "\n",
    "model_inputs = encoded_input.to('cuda')\n",
    "generated_ids = trained_model.generate(**model_inputs,\n",
    "                                    max_new_tokens=2048,\n",
    "                                    pad_token_id=tokenizer.eos_token_id)\n",
    "decoded_output = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b6fcc91888f4232845662ab087c5d1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>  [INST] perform the cell entity annotation (cea) task on this table: Charles and Theresa Roper House;historic house;1912-01-01;620 SW Alder Street, Newport, OR 97365, USA;Castellated Gothic|Frederick Armbruster Cottage;single-family detached home;1898-01-01;502 NE Tillamook Street, Portland, OR 97212, USA;Queen Anne style architecture|Charles and Theresa Roper House;folly;1912-01-01;620 SW Alder Street, Newport, OR 97365, USA;Castellated Gothic|Charles T. Holt House;single-family detached home;1897-01-01;228 Holt St., Haw River, North Carolina;Queen Anne style architecture. use the following pool of entities to annotate the table: Q5075307 [charles b. holt house];Q65920664 [nrhp nomination: frederick armbruster cottage];Q65920642 [frederick armbruster cottage];Q106617159 [single-family zoning];Q22449511 [castellated peaks];Q5079098 [charles holt];Q17560882 [charles t. holt house];Q7270218 [queen anne style];Q119192254 [alhambra tile works, newport, kentucky, usa];Q21450958 [armbruster];Q11576543 [tajiri historic house];Q56266149 [castellated gothic];Q1044392 [carolina beach, north carolina];Q116268157 [ne je ne dors ne je ne veille];Q77774445 [496, 498, 500, 502 union street, aberdeen];Q63985483 [folly];Q56122213 [nrhp nomination: charles and theresa roper house];Q27958714 [15 south second street, newport, pa];Q180174 [folly];Q54874680 [charles and theresa cornelius house];Q29787484 [single-family detached home rebhalde];Q529819 [queen anne style architecture];Q5773747 [historic house];Q2231747 [north river, north dakota];Q119190555 [ohio river, newport, kentucky, usa];Q5464799 [folly];Q64159387 [nehalem spit, tillamook county, oregon];Q2023027 [haw river, kawolin dinò];Q104007123 [queen anne style];Q1307276 [single-family detached home];Q6516654 [lefferts historic house];Q56121552 [charles and theresa roper house];Q49683965 [castellated ridge];[/INST] (1,0)=Q56121552 [charles and theresa roper house]|(1,2)=Q5773747 [historic house]|(1,3)=Q119192254 [alhambra tile works, newport, kentucky, usa]|(1,4)=Q56266149 [castellated gothic]|(2,0)=Q65920664 [nrhp nomination: frederick armbruster cottage]|(2,2)=Q1307276 [single-family detached home]|(2,3)=Q64159387 [nehalem spit, tillamook county, oregon]|(2,4)=Q56266149 [castellated\n"
     ]
    }
   ],
   "source": [
    "prompt = \"<s> [INST] perform the cell entity annotation (cea) task on this table: Charles and Theresa Roper House;historic house;1912-01-01;620 SW Alder Street, Newport, OR 97365, USA;Castellated Gothic|Frederick Armbruster Cottage;single-family detached home;1898-01-01;502 NE Tillamook Street, Portland, OR 97212, USA;Queen Anne style architecture|Charles and Theresa Roper House;folly;1912-01-01;620 SW Alder Street, Newport, OR 97365, USA;Castellated Gothic|Charles T. Holt House;single-family detached home;1897-01-01;228 Holt St., Haw River, North Carolina;Queen Anne style architecture. use the following pool of entities to annotate the table: Q5075307 [charles b. holt house];Q65920664 [nrhp nomination: frederick armbruster cottage];Q65920642 [frederick armbruster cottage];Q106617159 [single-family zoning];Q22449511 [castellated peaks];Q5079098 [charles holt];Q17560882 [charles t. holt house];Q7270218 [queen anne style];Q119192254 [alhambra tile works, newport, kentucky, usa];Q21450958 [armbruster];Q11576543 [tajiri historic house];Q56266149 [castellated gothic];Q1044392 [carolina beach, north carolina];Q116268157 [ne je ne dors ne je ne veille];Q77774445 [496, 498, 500, 502 union street, aberdeen];Q63985483 [folly];Q56122213 [nrhp nomination: charles and theresa roper house];Q27958714 [15 south second street, newport, pa];Q180174 [folly];Q54874680 [charles and theresa cornelius house];Q29787484 [single-family detached home rebhalde];Q529819 [queen anne style architecture];Q5773747 [historic house];Q2231747 [north river, north dakota];Q119190555 [ohio river, newport, kentucky, usa];Q5464799 [folly];Q64159387 [nehalem spit, tillamook county, oregon];Q2023027 [haw river, kawolin din\\u00f2];Q104007123 [queen anne style];Q1307276 [single-family detached home];Q6516654 [lefferts historic house];Q56121552 [charles and theresa roper house];Q49683965 [castellated ridge];[/INST]\"\n",
    "trained_model = AutoModelForCausalLM.from_pretrained(\"./wikidata_training/mistralai-sti-instruct_Pool_2\", quantization_config=bnb_config, device_map='auto', use_cache=False)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "def get_response(trained_model, tokenizer, prompt):\n",
    "    encoded_input = tokenizer(prompt, return_tensors=\"pt\", padding=True, add_special_tokens=False)\n",
    "    model_inputs = encoded_input.to('cuda')\n",
    "    generated_ids = trained_model.generate(**model_inputs,\n",
    "                                max_new_tokens=200,\n",
    "                                pad_token_id=tokenizer.eos_token_id)\n",
    "    decoded_output = tokenizer.batch_decode(generated_ids, skip_special_tokens=False)\n",
    "    print(decoded_output[0].replace(prompt, \"\"))\n",
    "\n",
    "get_response(trained_model, tokenizer, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c6c8c9e66f048cb8dbac999725625f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSTEDQ;Chemung County;1055.488;0.8008;(100000-999999)|St. Lawrence County;7373.772;4.95;(1000000-9999999)|Chenango County;2321.016;0.6042;(10000-99999)[/INST] (1,0)=Chemung County;(2,0)=St. Lawrence County;(3,0)=Chenango County;(1,1)=(100000-999999)|(2,1)=(1000000-9999999)|(3,1)=(10000-999\n"
     ]
    }
   ],
   "source": [
    "trained_model = AutoModelForCausalLM.from_pretrained(\"./mistralai-sti-instruct_Labels\", quantization_config=bnb_config, device_map='auto', use_cache=False)\n",
    "\n",
    "# Reload tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "prompt = \"<s>[INST] conduct the cell entity annotation (cea) task for wikidata on this table: Chemung Cownty;1055.488;0.8008|St. Lawrence Couny;7373.771999999999;4.95|Chenango ounty;2321.016;0.6042[/INST]</s>\"\n",
    "def stream(trained_model, tokenizer, prompt):\n",
    "    runtimeFlag = \"cuda:0\"\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(runtimeFlag)\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "    _ = trained_model.generate(**inputs, streamer=streamer, max_new_tokens=200)\n",
    "\n",
    "stream(trained_model, tokenizer, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
